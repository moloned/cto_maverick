# Xnor.ai acquisition by Apple (\#2)

## Interesting news of the day

Apple have reportedly acquired [xnor.ai](https://xnor.ai) for [$200M](https://techcrunch.com/2020/01/15/apple-buys-edge-based-ai-startup-xnor-ai-for-a-reported-200m/)

The company is well known for having advocated the use of binary neural networks rather than low-precision floating point, int8 or other numerical formats.

The advantage of (Binary Neural Networks) BNNs is that the chip area and propagation delays for circuits implementing BNNs are much less costly in dynamic and leakage power than NN accelerators using other formats.

The reason for this is down to Boolean algebra

The cost of a Binary multiplication is one AND gate

| A | B | A.B |
| - | - | --- |
| 0 | 0 | 0   |
| 0 | 1 | 0   |
| 1 | 0 | 0   |
| 1 | 1 | 1   |

Also the memory required to store a binary weight/coefficient for a BNN multiplier is one bit

This contrasts with the O(n<sup>2</sup>) logic gates required to implement an nxn bit binary multiplier

![Image result for 8x8 multiplier](/images/name/image1.jpeg)

And the wiring required to transmit the weight from a register or memory to the multiplier is one bit wide

In a nanometer world where wiring is now the most costly element of a System on Chip (SoC) this is a big deal

## Why this is a big deal

Binary networks are not a new idea and in some ways are almost as old as computing itself

Alan Turing proposed the use evolution to “train” a particular type of neural network he called a “B-type unorganised machine” in 1948

The challenge until now has that while BNNs are very attractive from a power/area/cost/frequency perspective this big challenge was the degradation in network accuracy

As we all know network accuracy is key to being able to deploy a product or service based upon it as 50% accuracy means you might as well flip a coin as pay heed to the output of your deep network

The jury has been out on whether it is possible to trade off an increase in the complexity of BNN networks against higher accuracy

By that I mean recover the accuracy lost in simply quantising all the weights to 1-bit by making much wider and potentially much deeper networks

This has been the focus of a lot of academic effort and a few startups over the past 3-4 years

Clearly this must have yielded fruit for Apple to have paid $200M

## The devil is in the details

While on the face of it there's a potential 100x reduction in the size of the multipliers

if 8x8 bit multipliers are used.

Equally the propagation delay is reduced by about one order of magnitude

Finally the wiring related to individual weight is reduced by a factor of 8

The real catch is how many binary multipliers it takes to achieve the desired accuracy

Indeed the size of the multipliers is not the only factor to consider but each additional binary multiplier has to be wired to a 1 bit memory

You require more storage as you have a higher number of multipliers You trade off the higher amount of weight storage and smaller multipliers against a lower number of larger multipliers

So binary networks trade multiplier size off against increased weight storage and wiring to transmit the weights from memory to the 1-bit multipliers (AND gates)

## Business advantage

Low cost, ultra low power inference for always-on features like wake on voice or wake on face are probably the key drivers for the acquisition by Apple.

Apparently Xnor’s business model was IP-enabled design services with several engineers being assigned to each project to perform the necessary integration and optimization work

This type of acquisition is not new to Apple with the company having previously having acquired custom dynamic logic startup [Intrinsity](https://www.anandtech.com/show/3665/apples-intrinsity-acquisition-winners-and-losers) in 2010 to bolster their SoC development teams for CPU and other high performance logic
